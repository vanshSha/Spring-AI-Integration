spring.application.name=ollama-spring-boot
spring.ai.ollama.chat.model=glm-5:cloud

# Use it when ollama is running on local system, Spring Boot and Ollama are on the same machine
spring.ai.ollama.chat.host=localhost

spring.ai.ollama.base-url=http://localhost:11434
# This is OLLAMA Server is running
# It connects your Spring Boot application to the local Ollama service.

